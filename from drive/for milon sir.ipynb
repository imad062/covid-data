{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths and general generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_base_path = \"./COVIDvsARDS/main/\"\n",
    "d2_base_path = \"./COVIDvsNORMAL/main/\"\n",
    "d3_base_path = \"./COVIDvsVBPNEUMONIA/main/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INIT_LR = 1e-3\n",
    "EPOCHS = 25\n",
    "BS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagesAndLabels(filepath, res):\n",
    "    \n",
    "    print(\"[INFO] loading images and labels...\")\n",
    "    imagePaths = list(paths.list_images(filepath))\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the image paths\n",
    "    for imagePath in imagePaths:\n",
    "        # extract the class label from the filename\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image, swap color channels, and resize it to be a fixed\n",
    "        # 224x224 pixels while ignoring aspect ratio\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (res, res))\n",
    "\n",
    "        # update the data and labels lists, respectively\n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "\n",
    "    # convert the data and labels to NumPy arrays while scaling the pixel\n",
    "    # intensities to the range [0, 255]\n",
    "    data = np.array(data) / 255.0\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # perform one-hot encoding on the labels\n",
    "    lb = LabelBinarizer()\n",
    "    labels = lb.fit_transform(labels)\n",
    "    labels = tf.keras.utils.to_categorical(labels)\n",
    "    \n",
    "    print(\"Data: \", end=\"\")\n",
    "    print(data.shape)\n",
    "    print(\"Labels: \", end=\"\")\n",
    "    print(labels.shape)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,                 \n",
    "                                                                horizontal_flip = True ,\n",
    "                                                                fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 16)          73744     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 144)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               37120     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 14,826,066\n",
      "Trainable params: 111,378\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_weightspath = \"./weights/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "vgg16baseModel = tf.keras.applications.VGG16(weights=None, include_top=False,input_shape=(256,256,3))\n",
    "\n",
    "vgg16baseModel.load_weights(vgg16_weightspath)\n",
    "\n",
    "x = vgg16baseModel.output\n",
    "x = tf.keras.layers.Conv2D(16, (3,3), activation='relu')(x)\n",
    "x = tf.keras.layers.MaxPooling2D(2,2)(x)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "for layer in vgg16baseModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg16_model = tf.keras.Model(vgg16baseModel.input, x)\n",
    "vgg16_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "baseModel = tf.keras.applications.VGG16(weights=\"imagenet\", include_top=False,input_shape=(256,256,3))\n",
    "\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = tf.keras.layers.AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = tf.keras.layers.Flatten(name=\"flatten\")(headModel)\n",
    "headModel = tf.keras.layers.Dense(64, activation=\"relu\")(headModel)\n",
    "headModel = tf.keras.layers.Dropout(0.5)(headModel)\n",
    "headModel = tf.keras.layers.Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = tf.keras.Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False\n",
    "\n",
    "# compile our model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = tf.keras.optimizers.Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2 normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images and labels...\n",
      "Data: (156, 256, 256, 3)\n",
      "Labels: (156, 2)\n"
     ]
    }
   ],
   "source": [
    "d2_images, d2_labels = getImagesAndLabels(d2_base_path, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(d2_images, d2_labels,\n",
    "\ttest_size=0.20, stratify=d2_labels, random_state=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 224, 224, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 224, 224, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training head...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 15 steps, validate on 32 samples\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 56s 4s/step - loss: 0.8419 - accuracy: 0.4483 - val_loss: 0.6868 - val_accuracy: 0.5938\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 62s 4s/step - loss: 0.7584 - accuracy: 0.4828 - val_loss: 0.6852 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 60s 4s/step - loss: 0.7327 - accuracy: 0.4655 - val_loss: 0.6883 - val_accuracy: 0.5312\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 60s 4s/step - loss: 0.7112 - accuracy: 0.4741 - val_loss: 0.6675 - val_accuracy: 0.7812\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 60s 4s/step - loss: 0.7249 - accuracy: 0.5000 - val_loss: 0.6701 - val_accuracy: 0.6250\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 68s 5s/step - loss: 0.7031 - accuracy: 0.5603 - val_loss: 0.6795 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 65s 4s/step - loss: 0.7207 - accuracy: 0.5086 - val_loss: 0.6726 - val_accuracy: 0.7188\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 98s 7s/step - loss: 0.6958 - accuracy: 0.5172 - val_loss: 0.6736 - val_accuracy: 0.7500\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 89s 6s/step - loss: 0.6917 - accuracy: 0.5000 - val_loss: 0.6749 - val_accuracy: 0.7188\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 82s 5s/step - loss: 0.6969 - accuracy: 0.5172 - val_loss: 0.6787 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit_generator(\n",
    "\ttrain_datagen.flow(trainX, trainY, batch_size=BS),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tvalidation_steps=len(testX) // BS,\n",
    "\tepochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
